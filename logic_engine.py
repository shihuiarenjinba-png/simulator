import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf
import statsmodels.api as sm
from statsmodels.regression.rolling import RollingOLS
from sklearn.decomposition import PCA
import pandas_datareader.data as web
from datetime import datetime

# =========================================================
# ğŸ› ï¸ Class Definitions (Structure Fixed for Import Safety)
# =========================================================

class MarketDataEngine:
    """Manages market data, factors, and benchmarks."""
    def __init__(self):
        self.start_date = "2000-01-01"
        self.end_date = datetime.today().strftime('%Y-%m-%d')
        self.usdjpy_cache = None

    def validate_tickers(self, input_dict):
        """Check if tickers exist."""
        valid_data = {}
        invalid_tickers = []
        status_text = st.empty()
        
        for ticker, weight in input_dict.items():
            try:
                # Check via yfinance
                tick = yf.Ticker(ticker)
                hist = tick.history(period="5d")
                if not hist.empty:
                    valid_data[ticker] = {'name': ticker, 'weight': weight}
                    status_text.text(f"âœ… OK: {ticker}")
                else:
                    invalid_tickers.append(ticker)
            except:
                invalid_tickers.append(ticker)
        
        status_text.empty()
        return valid_data, invalid_tickers

    def _get_usdjpy(self):
        """Fetch JPY rate with cache."""
        if self.usdjpy_cache is not None:
            return self.usdjpy_cache
        try:
            raw = yf.download("JPY=X", start=self.start_date, end=self.end_date, interval="1mo", auto_adjust=True, progress=False)
            
            if isinstance(raw, pd.DataFrame):
                if 'Close' in raw.columns:
                    usdjpy = raw['Close']
                else:
                    usdjpy = raw.iloc[:, 0]
            else:
                usdjpy = raw

            if isinstance(usdjpy, pd.DataFrame):
                usdjpy = usdjpy.iloc[:, 0]

            # ä¿®æ­£: 'M' -> 'ME' (FutureWarningå¯¾ç­–)
            usdjpy = usdjpy.resample('ME').last().ffill()
            if usdjpy.index.tz is not None: 
                usdjpy.index = usdjpy.index.tz_localize(None)
            
            self.usdjpy_cache = usdjpy
            return usdjpy
        except Exception:
            return pd.Series(dtype=float)

    @st.cache_data(ttl=3600*24*7)
    def fetch_french_factors(_self, region='US'):
        """Fetch Fama-French Factors (Robust Fallback)."""
        try:
            name = 'F-F_Research_Data_Factors'
            if region == 'Japan': 
                name = 'Japan_3_Factors'
            elif region == 'Global': 
                name = 'Global_3_Factors'

            # Attempt to fetch data
            ff_data = web.DataReader(name, 'famafrench', start=_self.start_date, end=_self.end_date)[0]
            
            # Process data if successful
            ff_data = ff_data / 100.0
            # ä¿®æ­£: 'M' -> 'ME'
            ff_data.index = ff_data.index.to_timestamp(freq='ME')
            
            if ff_data.index.tz is not None: 
                ff_data.index = ff_data.index.tz_localize(None)
            
            return ff_data
        except Exception:
            return pd.DataFrame()

    @st.cache_data(ttl=3600*24)
    def fetch_historical_prices(_self, tickers):
        """Fetch stock prices."""
        try:
            raw_data = yf.download(tickers, start=_self.start_date, end=_self.end_date, interval="1mo", auto_adjust=True, progress=False)
            data = pd.DataFrame()

            if len(tickers) == 1:
                ticker = tickers[0]
                if isinstance(raw_data, pd.Series):
                    data[ticker] = raw_data
                elif isinstance(raw_data, pd.DataFrame):
                    if 'Close' in raw_data.columns:
                        data[ticker] = raw_data['Close']
                    else:
                        data[ticker] = raw_data.iloc[:, 0]
            else:
                if isinstance(raw_data.columns, pd.MultiIndex):
                    try:
                        data = raw_data.xs('Close', axis=1, level=0, drop_level=True)
                    except KeyError:
                        try:
                            data = raw_data.xs('Adj Close', axis=1, level=0, drop_level=True)
                        except:
                            data = raw_data.iloc[:, :len(tickers)]
                            data.columns = tickers
                else:
                    data = raw_data

            # ä¿®æ­£: 'M' -> 'ME'
            data = data.resample('ME').last().ffill()
            if data.index.tz is not None:
                data.index = data.index.tz_localize(None)

            usdjpy = _self._get_usdjpy()
            if not usdjpy.empty:
                usdjpy = usdjpy.reindex(data.index, method='ffill')
                data_jpy = data.copy()
                for col in data.columns:
                    # Do not convert Japanese assets or indices
                    is_japan = str(col).endswith(".T") or str(col) in ["^N225", "^TPX", "1306.T"]
                    if not is_japan:
                        data_jpy[col] = data[col] * usdjpy
            else:
                data_jpy = data

            returns = data_jpy.pct_change().dropna(how='all').dropna()
            
            valid_cols = [c for c in returns.columns if c in tickers]
            if valid_cols:
                returns = returns[valid_cols]
            
            return returns
        except Exception as e:
            st.error(f"Data Fetch Error: {e}")
            return pd.DataFrame()

    @st.cache_data(ttl=3600*24)
    def fetch_benchmark_data(_self, ticker, is_jpy_asset=False):
        """Fetch benchmark."""
        try:
            raw_data = yf.download(ticker, start=_self.start_date, end=_self.end_date, interval="1mo", auto_adjust=True, progress=False)
            data = pd.Series(dtype=float)
            if isinstance(raw_data, pd.DataFrame):
                if 'Close' in raw_data.columns:
                    data = raw_data['Close']
                elif isinstance(raw_data.columns, pd.MultiIndex):
                     try: data = raw_data.xs('Close', axis=1, level=0, drop_level=True)
                     except: data = raw_data.iloc[:, 0]
                else:
                    data = raw_data.iloc[:, 0]
            else:
                data = raw_data

            if isinstance(data, pd.DataFrame):
                data = data.iloc[:, 0]

            # ä¿®æ­£: 'M' -> 'ME'
            data = data.resample('ME').last().ffill()
            if data.index.tz is not None:
                data.index = data.index.tz_localize(None)

            if not is_jpy_asset:
                usdjpy = _self._get_usdjpy()
                if not usdjpy.empty:
                    usdjpy = usdjpy.reindex(data.index, method='ffill')
                    data = data * usdjpy
            
            return data.pct_change().dropna()
        except:
            return pd.Series(dtype=float)

class PortfolioAnalyzer:
    
    @staticmethod
    def create_synthetic_history(returns_df, weights_dict):
        valid_tickers = [t for t in weights_dict.keys() if t in returns_df.columns]
        if not valid_tickers:
            return pd.Series(dtype=float), {}

        filtered_weights = {k: weights_dict[k] for k in valid_tickers}
        total_weight = sum(filtered_weights.values())
        norm_weights = {k: v/total_weight for k, v in filtered_weights.items()}
        
        weighted_returns = pd.DataFrame()
        for ticker, w in norm_weights.items():
            weighted_returns[ticker] = returns_df[ticker] * w
            
        port_ret = weighted_returns.sum(axis=1)
        return port_ret, norm_weights

    @staticmethod
    def calculate_correlation_matrix(returns_df):
        if returns_df.empty:
            return pd.DataFrame()
        return returns_df.corr()

    @staticmethod
    def perform_factor_regression(port_ret, factor_df):
        if port_ret.empty or factor_df is None or factor_df.empty:
            return None, None

        df_y = port_ret.to_frame(name='y')
        df_y['period'] = df_y.index.to_period('M') 
        df_x = factor_df.copy()
        df_x['period'] = df_x.index.to_period('M') 
        
        merged = pd.merge(df_y, df_x, on='period', how='inner').dropna()
        if merged.empty: return None, None
        
        y = merged['y']
        X_cols = [c for c in merged.columns if c in ['Mkt-RF', 'SMB', 'HML']]
        if not X_cols: return None, None
        
        X = merged[X_cols]
        X = sm.add_constant(X)

        try:
            model = sm.OLS(y, X)
            results = model.fit()
            return results.params, results.rsquared
        except:
            return None, None

    @staticmethod
    def run_monte_carlo_simulation(port_ret, n_years=20, n_simulations=5000, initial_investment=1000000):
        """
        ä¿®æ­£: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå›æ•°ã‚’7500 -> 5000ã«å¤‰æ›´
        """
        if port_ret.empty:
            return None, None

        mu_monthly = port_ret.mean()
        sigma_monthly = port_ret.std()
        
        n_months = n_years * 12
        drift = (mu_monthly - 0.5 * sigma_monthly**2)
        
        df_t = 6
        Z = np.random.standard_t(df_t, (n_months, n_simulations))
        
        daily_returns = np.exp(drift + sigma_monthly * Z)
        
        price_paths = np.zeros((n_months + 1, n_simulations))
        price_paths[0] = initial_investment
        price_paths[1:] = initial_investment * np.cumprod(daily_returns, axis=0)
        
        last_date = port_ret.index[-1]
        # ä¿®æ­£: 'M' -> 'ME'
        future_dates = pd.date_range(start=last_date, periods=n_months + 1, freq='ME')
        
        percentiles = [10, 50, 90]
        stats_data = np.percentile(price_paths, percentiles, axis=1)
        df_stats = pd.DataFrame(stats_data.T, index=future_dates, columns=['p10', 'p50', 'p90'])
        
        final_values = price_paths[-1, :]
        
        return df_stats, final_values

    @staticmethod
    def calculate_calmar_ratio(port_ret):
        if port_ret.empty: return np.nan
        cum_ret = (1 + port_ret).cumprod()
        if len(port_ret) < 12: return np.nan
        cagr = (cum_ret.iloc[-1])**(12/len(port_ret)) - 1
        max_dd = (cum_ret / cum_ret.cummax() - 1).min()
        if max_dd == 0: return np.nan
        return cagr / abs(max_dd)

    @staticmethod
    def calculate_omega_ratio(port_ret, threshold=0.0):
        if port_ret.empty: return np.nan
        gains = port_ret[port_ret > threshold] - threshold
        losses = threshold - port_ret[port_ret < threshold]
        sum_gains = gains.sum()
        sum_losses = losses.sum()
        if sum_losses == 0: return np.inf
        return sum_gains / sum_losses

    @staticmethod
    def calculate_information_ratio(port_ret, bench_ret):
        if port_ret.empty or bench_ret.empty: return np.nan, np.nan
        
        p_df = port_ret.to_frame(name='p')
        b_df = bench_ret.to_frame(name='b')
        p_df['period'] = p_df.index.to_period('M')
        b_df['period'] = b_df.index.to_period('M')
        
        merged = pd.merge(p_df, b_df, on='period', how='inner').dropna()
        
        if len(merged) < 12: return np.nan, np.nan
        
        active_ret = merged['p'] - merged['b']
        mean_active = active_ret.mean() * 12
        tracking_error = active_ret.std() * np.sqrt(12)
        if tracking_error == 0: return np.nan, 0.0
        return mean_active / tracking_error, tracking_error

    @staticmethod
    def perform_pca(returns_df):
        if returns_df.shape[1] < 2: 
            return 1.0, None
        
        pca = PCA(n_components=2)
        pca.fit(returns_df)
        
        loadings = pd.DataFrame(
            pca.components_.T, 
            index=returns_df.columns, 
            columns=['PC1', 'PC2']
        )
        
        return pca.explained_variance_ratio_[0], loadings

    @staticmethod
    def rolling_beta_analysis(port_ret, factor_df, window=24):
        if factor_df is None or factor_df.empty or port_ret.empty:
            return pd.DataFrame()

        df_y = port_ret.to_frame(name='y')
        df_y['period'] = df_y.index.to_period('M') 
        df_x = factor_df.copy()
        df_x['period'] = df_x.index.to_period('M') 
        
        merged = pd.merge(df_y, df_x, on='period', how='inner').dropna()
        if merged.empty: return pd.DataFrame()
        
        y = merged['y']
        X_cols = [c for c in merged.columns if c not in ['y', 'period']]
        X = merged[X_cols]
        
        data_len = len(y)
        if data_len < window:
            window = max(6, int(data_len / 2))
        if data_len < window:
            return pd.DataFrame()

        try:
            X_const = sm.add_constant(X)
            model = RollingOLS(y, X_const, window=window)
            rres = model.fit()
            params = rres.params.copy()
            if 'const' in params.columns:
                params = params.drop(columns=['const'])
            return params.dropna()
        except:
            return pd.DataFrame()

    @staticmethod
    def cost_drag_simulation(port_ret, cost_tier):
        """
        Calculates impact of costs.
        FIXED: Returns exactly 4 values to match app.py expectation.
        Returns: Gross Curve, Net Curve, Loss Amount, Loss Percentage
        """
        if port_ret.empty: return pd.Series(), pd.Series(), 0, 0
        
        cost_map = {'Low': 0.001, 'Medium': 0.006, 'High': 0.020}
        annual_cost = cost_map.get(cost_tier, 0.006)
        monthly_cost = (1 + annual_cost)**(1/12) - 1
        
        net_ret = port_ret - monthly_cost
        gross_cum = (1 + port_ret).cumprod()
        net_cum = (1 + net_ret).cumprod()
        
        final_gross = gross_cum.iloc[-1]
        final_net = net_cum.iloc[-1]
        
        diff_val = final_gross - final_net
        lost_pct = 1 - (final_net / final_gross) 
        
        # ä¿®æ­£: æˆ»ã‚Šå€¤ã‚’4ã¤ã«çµ±ä¸€ (unpacking errorå›é¿)
        return gross_cum, net_cum, diff_val, lost_pct

    @staticmethod
    def calculate_strict_attribution(returns_df, weights_dict):
        assets = list(weights_dict.keys())
        available_assets = [a for a in assets if a in returns_df.columns]
        if not available_assets: return pd.Series(dtype=float)
            
        w_series = pd.Series(weights_dict)
        total_w = w_series[available_assets].sum()
        initial_w = w_series[available_assets] / total_w
        
        r_df = returns_df[available_assets].copy()
        
        cum_r_index = (1 + r_df).cumprod()
        asset_values = cum_r_index.multiply(initial_w, axis=1)
        port_values = asset_values.sum(axis=1)
        
        weights_df = asset_values.div(port_values, axis=0).shift(1)
        weights_df.iloc[0] = initial_w
        
        port_ret = (weights_df * r_df).sum(axis=1)
        total_cum_ret = (1 + port_ret).prod() - 1
        
        log_return = np.log(1 + total_cum_ret)
        k = log_return / total_cum_ret if total_cum_ret != 0 else 1.0
            
        kt = np.log(1 + port_ret) / port_ret
        kt = kt.fillna(1.0)
        
        term = weights_df * r_df
        smoothed_term = term.multiply(kt, axis=0)
        
        final_attribution = smoothed_term.sum() / k
        
        return final_attribution.sort_values(ascending=True)

    @staticmethod
    def calculate_risk_contribution(returns_df, weights_dict):
        """
        Calculates Marginal Risk Contribution (MRC).
        """
        assets = list(weights_dict.keys())
        valid_assets = [a for a in assets if a in returns_df.columns]
        if not valid_assets:
            return pd.Series(dtype=float)

        # Filter and Normalize weights
        w_series = pd.Series({k: weights_dict[k] for k in valid_assets})
        w_series = w_series / w_series.sum() 
        
        # Covariance Matrix (Annualized)
        cov_matrix = returns_df[valid_assets].cov() * 12 
        
        # Portfolio Volatility
        port_vol = np.sqrt(w_series.T @ cov_matrix @ w_series)
        
        # Marginal Risk Contribution: (Cov * w) / PortVol
        mrc = cov_matrix @ w_series / port_vol
        
        # Risk Contribution: w * MRC
        rc = w_series * mrc
        
        # Percent Contribution: RC / PortVol
        rc_pct = rc / port_vol
        
        # ä¿®æ­£: sort_valuesã‚’å‰Šé™¤ã—ã€æŠ•è³‡æ¯”ç‡ã¨åŒã˜ä¸¦ã³é †ã‚’ç¶­æŒ
        return rc_pct

    @staticmethod
    def calculate_label_offsets(values, min_dist=0.08, base_y=1.05):
        """
        Calculates Y-axis offsets for histogram labels to prevent overlap.
        """
        if not values: return []
        
        # Create a list of (index, value)
        indexed_values = sorted(enumerate(values), key=lambda x: x[1])
        
        y_offsets = [base_y] * len(values)
        
        # Determine value range to normalize distance check
        val_range = max(values) - min(values)
        if val_range == 0: val_range = 1.0
        
        # Iterate through sorted values and stack levels if too close
        levels = [base_y] * len(values) # temporary storage for sorted
        current_level = base_y
        
        for i in range(1, len(indexed_values)):
            curr_val = indexed_values[i][1]
            prev_val = indexed_values[i-1][1]
            
            # Check normalized distance
            dist = (curr_val - prev_val) / val_range
            
            if dist < min_dist:
                # If close to previous, bump up level
                prev_level = levels[i-1]
                if prev_level == base_y:
                    current_level = base_y + 0.15
                elif prev_level == base_y + 0.15:
                    current_level = base_y + 0.3
                else:
                    current_level = base_y # Reset if stack gets too high
            else:
                current_level = base_y
            
            levels[i] = current_level
            
        # Map back to original indices
        final_offsets = [0.0] * len(values)
        for i, (orig_idx, _) in enumerate(indexed_values):
            final_offsets[orig_idx] = levels[i]
            
        return final_offsets

class PortfolioDiagnosticEngine:
    @staticmethod
    def generate_report(weights_dict, pca_ratio, port_ret, benchmark_ret=None):
        report = {
            "type": "",
            "risk_comment": "",
            "diversification_comment": "",
            "action_plan": ""
        }
        
        num_assets = len(weights_dict)
        
        if num_assets == 1:
            report["type"] = "ğŸ¹ é›†ä¸­æŠ•è³‡ (ã‚¹ãƒŠã‚¤ãƒ‘ãƒ¼å‹)"
            report["diversification_comment"] = "åˆ†æ•£åŠ¹æœã¯ã‚¼ãƒ­ã§ã™ã€‚ã™ã¹ã¦ã®åµã‚’ä¸€ã¤ã®ã‚«ã‚´ã«å…¥ã‚Œã¦ã„ã¾ã™ã€‚"
            report["risk_comment"] = "âš ï¸ å€‹åˆ¥éŠ˜æŸ„ãƒªã‚¹ã‚¯ã‚’æœ€å¤§é™ã«è² ã£ã¦ã„ã¾ã™ã€‚"
            report["action_plan"] = "å°‘ãªãã¨ã‚‚3ã€œ5ã¤ã®ç›¸é–¢ã®ä½ã„è³‡ç”£ã«åˆ†æ•£ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
        else:
            if pca_ratio >= 0.85:
                report["type"] = "âš ï¸ è¦‹ã›ã‹ã‘ã®åˆ†æ•£ (ãƒ•ã‚§ã‚¤ã‚¯ãƒ»ãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³)"
                report["diversification_comment"] = f"å¤‰å‹•ã®{pca_ratio*100:.1f}%ãŒå˜ä¸€ã®è¦å› ï¼ˆå¸‚å ´å…¨ä½“ãªã©ï¼‰ã§èª¬æ˜ã•ã‚Œã¦ã—ã¾ã„ã¾ã™ã€‚"
                report["risk_comment"] = "å¸‚å ´æš´è½æ™‚ã«ã€ä¿æœ‰è³‡ç”£ã™ã¹ã¦ãŒåŒæ™‚ã«ä¸‹è½ã™ã‚‹ãƒªã‚¹ã‚¯ãŒé«˜ã„çŠ¶æ…‹ã§ã™ã€‚"
                report["action_plan"] = "æ ªå¼ä»¥å¤–ã®è³‡ç”£ï¼ˆå‚µåˆ¸ã€ã‚´ãƒ¼ãƒ«ãƒ‰ãªã©ï¼‰ã‚’è¿½åŠ ã—ã€ãƒªã‚¹ã‚¯è¦å› ã‚’åˆ†æ•£ã—ã¦ãã ã•ã„ã€‚"
            elif pca_ratio <= 0.60:
                report["type"] = "ğŸ° è¦å¡å‹ (ãƒ•ã‚©ãƒ¼ãƒˆãƒ¬ã‚¹)"
                report["diversification_comment"] = f"ãƒ¡ã‚¤ãƒ³è¦å› ã«ã‚ˆã‚‹èª¬æ˜ç‡ã¯{pca_ratio*100:.1f}%ã«ç•™ã¾ã‚Šã€ç‹¬è‡ªã®å‹•ãã‚’ã™ã‚‹è³‡ç”£ãŒçµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚"
                report["risk_comment"] = "ç„¡é§„ãªãƒªã‚¹ã‚¯ãŒåŠ¹æœçš„ã«åˆ†æ•£ã•ã‚Œã€é˜²å¾¡åŠ›ãŒé«˜ã„ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã§ã™ã€‚"
                report["action_plan"] = "ç¾åœ¨ã®ãƒãƒ©ãƒ³ã‚¹ã¯éå¸¸ã«è‰¯å¥½ã§ã™ã€‚ãƒªãƒãƒ©ãƒ³ã‚¹ã‚’è¡Œã„ç¶­æŒã—ã¾ã—ã‚‡ã†ã€‚"
            else:
                report["type"] = "âš–ï¸ ãƒãƒ©ãƒ³ã‚¹å‹"
                report["diversification_comment"] = f"å¸‚å ´é€£å‹•æ€§ã¯{pca_ratio*100:.1f}%ã§ã€æ¨™æº–çš„ãªåˆ†æ•£ãƒ¬ãƒ™ãƒ«ã§ã™ã€‚"
                report["risk_comment"] = "å¸‚å ´å¹³å‡ã¨åŒç¨‹åº¦ã®ãƒªã‚¹ã‚¯ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ç‰¹æ€§ã‚’æŒã¤å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚"
                report["action_plan"] = "ã‚ˆã‚Šé˜²å¾¡åŠ›ã‚’é«˜ã‚ã‚‹ãªã‚‰ã€å‚µåˆ¸æ¯”ç‡ã®èª¿æ•´ã‚„ã‚ªãƒ«ã‚¿ãƒŠãƒ†ã‚£ãƒ–è³‡ç”£ã®æ¤œè¨ãŒæœ‰åŠ¹ã§ã™ã€‚"

        return report

    @staticmethod
    def get_skew_kurt_desc(port_ret):
        if port_ret.empty: return "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã§ã™ã€‚"
        skew = port_ret.skew()
        kurt = port_ret.kurt()
        desc = []
        if skew < -0.5: desc.append("âš ï¸ è² ã®æ­ªåº¦: é€šå¸¸æ™‚ã¯å®‰å®šã—ã¦ã„ã¾ã™ãŒã€ç¨€ã«å¤§ããªæ€¥è½ãŒèµ·ãã‚‹ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ï¼ˆã‚³ãƒ„ã‚³ãƒ„ãƒ‰ã‚«ãƒ³å‹ï¼‰ã€‚")
        elif skew > 0.5: desc.append("âœ… æ­£ã®æ­ªåº¦: æå¤±ã¯é™å®šçš„ã§ã™ãŒã€ç¨€ã«å¤§ããªåˆ©ç›ŠãŒå‡ºã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        
        if kurt > 2.0: desc.append("âš ï¸ ãƒ•ã‚¡ãƒƒãƒˆãƒ†ãƒ¼ãƒ«: æ­£è¦åˆ†å¸ƒã«æ¯”ã¹ã¦ã€Œæ¥µç«¯ãªäº‹è±¡ï¼ˆæš´é¨°ãƒ»æš´è½ï¼‰ã€ãŒç™ºç”Ÿã™ã‚‹ç¢ºç‡ãŒé«˜ã„çŠ¶æ…‹ã§ã™ã€‚")
        
        return " ".join(desc) if desc else "çµ±è¨ˆçš„ã«æ¨™æº–çš„ãªåˆ†å¸ƒï¼ˆæ­£è¦åˆ†å¸ƒã«è¿‘ã„ï¼‰ã§ã™ã€‚"

    @staticmethod
    def generate_factor_report(params):
        """Translate Factor Analysis."""
        if params is None: return "ãƒ‡ãƒ¼ã‚¿ãªã—"
        
        comments = []
        
        # 1. HML
        hml = params.get('HML', 0)
        if hml > 0.15:
            comments.append("âœ… **ãƒãƒªãƒ¥ãƒ¼æ ªé¸å¥½:** å‰²å®‰æ ªã‚„é«˜é…å½“æ ªã¨ã®é€£å‹•æ€§ãŒé«˜ã„ã§ã™ã€‚")
        elif hml < -0.15:
            comments.append("ğŸš€ **ã‚°ãƒ­ãƒ¼ã‚¹æ ªé¸å¥½:** æˆé•·æ ªã‚„ãƒã‚¤ãƒ†ã‚¯æ ªã¨ã®é€£å‹•æ€§ãŒé«˜ã„ã§ã™ã€‚")
        else:
            comments.append("âš–ï¸ **ã‚¹ã‚¿ã‚¤ãƒ«ä¸­ç«‹:** ãƒãƒªãƒ¥ãƒ¼ã¨ã‚°ãƒ­ãƒ¼ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã¾ã™ã€‚")

        # 2. SMB
        smb = params.get('SMB', 0)
        if smb > 0.15:
            comments.append("ğŸ£ **å°å‹æ ªãƒã‚¤ã‚¢ã‚¹:** å¤‰å‹•ã¯å¤§ãã„ã§ã™ãŒã€å°†æ¥ã®æˆé•·ä½™åœ°ã‚’å–ã‚Šã«è¡Œã£ã¦ã„ã¾ã™ã€‚")
        elif smb < -0.15:
            comments.append("ğŸ˜ **å¤§å‹æ ªãƒã‚¤ã‚¢ã‚¹:** å®‰å®šã—ãŸå¤§ä¼æ¥­ä¸­å¿ƒã®æ§‹æˆã§ã™ã€‚")
        
        # 3. Mkt-RF
        mkt = params.get('Mkt-RF', 1.0)
        if mkt > 1.1:
            comments.append("ğŸ¢ **ãƒã‚¤ãƒ™ãƒ¼ã‚¿ï¼ˆç©æ¥µé‹ç”¨ï¼‰:** å¸‚å ´å¹³å‡ã‚ˆã‚Šã‚‚å¤§ããå‹•ãã€æ”»æ’ƒçš„ãªæ§‹æˆã§ã™ã€‚")
        elif mkt < 0.9:
            comments.append("ğŸ›¡ï¸ **ãƒ­ãƒ¼ãƒ™ãƒ¼ã‚¿ï¼ˆå®ˆã‚Šã®é‹ç”¨ï¼‰:** å¸‚å ´ä¸‹è½æ™‚ã«ã‚‚æ¯”è¼ƒçš„ãƒ€ãƒ¡ãƒ¼ã‚¸ã‚’å—ã‘ã«ãã„æ§‹æˆã§ã™ã€‚")

        return "\n".join(comments)
